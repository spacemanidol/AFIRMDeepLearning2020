{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up a Simple Neural Network from Scratch\n",
    "\n",
    "In this demo we will code up a simple multi-layer neural network with single input feature, single output, and single neuron per layer using (i) PyTorch and (ii) from scratch. We will demonstrate that both approaches produce _identical_ outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple DNN in PyTorch\n",
    "We begin by first implementing a simple neural network with $n$ layers in PyTorch. The model leverages the default **Linear** and **Tanh** layers available under _torch.nn_ namespace.\n",
    "\n",
    "Note: PyTorch can automatically compute the gradients for backpropagation. So, we only need to define the forward function for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class PyDNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super(PyDNN, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(n):                          # define a sequence of linear and non-linear layers\n",
    "            self.layers.append(nn.Linear(1, 1))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*self.layers)    # wrapping it for easier registration and invocation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple DNN from scratch\n",
    "Next, we re-implement the same model _without_ using PyTorch. Note, that now we need to define both the forward and backward methods. The forward method is similar to the forward method in our previously defined PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDNN:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.layers = []\n",
    "        for i in range(n):                              # define a sequence of linear and non-linear layers\n",
    "            self.layers.append(OurLinear())\n",
    "            self.layers.append(OurTanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:                       # run layers sequentially\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_out = layer.backward(grad_out, lr)     # backpropagate gradients through the layers\n",
    "        return grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our DNN model makes use of two classes **OurLinear** and **OurTanh**â€”which we implement next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple linear layer from scratch\n",
    "Our linear layer computes the following function in forward pass:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y=w.x+b\"/>\n",
    "\n",
    "The corresponding gradients for backpropagation are computed as follows:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\frac{\\delta \\ell}{\\delta w}=\\frac{\\delta \\ell}{\\delta y}\\times\\frac{\\delta y}{\\delta w} = \\frac{\\delta \\ell}{\\delta y}\\times x\"/>\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\frac{\\delta \\ell}{\\delta b}=\\frac{\\delta \\ell}{\\delta y}\\times\\frac{\\delta y}{\\delta b} = \\frac{\\delta \\ell}{\\delta y}\\times 1\"/>\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\frac{\\delta \\ell}{\\delta x}=\\frac{\\delta \\ell}{\\delta y}\\times\\frac{\\delta y}{\\delta x} = \\frac{\\delta \\ell}{\\delta y}\\times w\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class OurLinear:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weight = random.uniform(-1, 1)     # randomly initialize learnable weight\n",
    "        self.bias = random.uniform(-1, 1)       # randomly initialize learnable bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x                              # save the input for gradient computation later\n",
    "        out = x * self.weight + self.bias       # compute output\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        grad_w = grad_out * self.x              # compute gradients w.r.t. weight\n",
    "        grad_b = grad_out                       # compute gradients w.r.t. bias\n",
    "        grad_in = grad_out * self.weight        # compute gradients w.r.t. input\n",
    "        self.weight -= lr * grad_w              # update weight\n",
    "        self.bias -= lr * grad_b                # update bias\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple tanh layer from scratch\n",
    "Our tanh layer computes the following function in forward pass:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;y=tanh(x)\"/>\n",
    "\n",
    "The corresponding gradients for backpropagation are computed as follows:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\frac{\\delta \\ell}{\\delta x}=\\frac{\\delta \\ell}{\\delta y}\\times\\frac{\\delta y}{\\delta x} = \\frac{\\delta \\ell}{\\delta y}\\times \\big(1 - tanh^2(x)\\big)\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTanh:\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.y = np.tanh(x)                     # compute and save output for gradient computation later\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        grad_in = grad_out * (1 - self.y**2)    # compute gradients w.r.t. input\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple mean squared loss from scratch\n",
    "We also need to implement our mean squared error (MSE) loss. PyTorch implements its own MSE loss under torch.nn that we will use for the PyTorch model.\n",
    "\n",
    "Our MSE loss computes the following function in forward pass:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\ell=(y-x)^2\"/>\n",
    "\n",
    "The corresponding gradients for backpropagation are computed as follows:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\Large&space;\\frac{\\delta \\ell}{\\delta x}=-2 \\times (y - x)\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurMSELoss:\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.z = (y - x)        # save the intermediate for gradient computation later\n",
    "        loss = self.z**2        # compute output\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        grad_in = -2 * self.z   # compute gradients w.r.t. input\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function we want to learn\n",
    "Next, we define an arbitrary function $f(x)$ that we want to model. This is the ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function_we_want_to_learn(x):\n",
    "    return np.tanh(0.6 * x - 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple dataset\n",
    "We define a synthetic dataset by randomly sampling $x$ uniformly between $[-1, 1]$ and computing $y=f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class SimpleDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, func, num_samples=50):\n",
    "        super(SimpleDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        xs = [random.uniform(-1, 1) for i in range(self.num_samples)]   # create random values for x\n",
    "        xys = [([x], [func(x)]) for x in xs]                            # compute y for each x\n",
    "        xys = [(np.asarray(x, dtype=np.float32),\n",
    "                np.asarray(y, dtype=np.float32)) for (x, y) in xys]     # convert to numpy array\n",
    "        self.samples = [(torch.from_numpy(x),            \n",
    "                         torch.from_numpy(y)) for (x, y) in xys]        # convert to torch tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to get and set DNN parameters\n",
    "We define some utility methods here for getting, copying, and displaying model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(dnn):\n",
    "    params = {}\n",
    "    params['ws'] = []\n",
    "    params['bs'] = []\n",
    "    for layer in dnn.layers:\n",
    "        if isinstance(layer, OurLinear):\n",
    "            params['ws'].append(layer.weight)\n",
    "            params['bs'].append(layer.bias)\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            params['ws'].append(layer.weight.item())\n",
    "            params['bs'].append(layer.bias.item())\n",
    "    return params\n",
    "\n",
    "def copy_params(py_dnn, our_dnn):\n",
    "    n = len(py_dnn.layers)\n",
    "    for i in range(n):\n",
    "        if isinstance(py_dnn.layers[i], nn.Linear):\n",
    "            our_dnn.layers[i].weight = py_dnn.layers[i].weight.item()\n",
    "            our_dnn.layers[i].bias = py_dnn.layers[i].bias.item()\n",
    "\n",
    "def format_params(params):\n",
    "    return 'params = {}'.format({k : [round(x, 3) for x in v] for k, v in params.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data loader and models\n",
    "We instantiate the dataloader, the PyTorch DNN model and our DNN model implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n = 1               # number of layers\n",
    "lr = 0.01           # learning rate\n",
    "epoch_size = 50     # number of samples per epoch\n",
    "num_epochs = 10     # number of epochs\n",
    "\n",
    "dataset = SimpleDataset(some_function_we_want_to_learn, num_samples = epoch_size * num_epochs)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "py_dnn = PyDNN(n)\n",
    "our_dnn = OurDNN(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize both models to the same random start state\n",
    "We can comment this out if we want the two model parameters to be randomly initialized independently of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_params(py_dnn, our_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PyDNN model\n",
    "Finally, we train the DNN model implemented using PyTorch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before training]\tparams = {'ws': [0.547], 'bs': [-0.399]}\n",
      "[after batch 50] loss = 0.021 params = {'ws': [0.513], 'bs': [-0.58]}\n",
      "[after batch 100] loss = 0.019 params = {'ws': [0.501], 'bs': [-0.677]}\n",
      "[after batch 150] loss = 0.005 params = {'ws': [0.488], 'bs': [-0.736]}\n",
      "[after batch 200] loss = 0.003 params = {'ws': [0.487], 'bs': [-0.775]}\n",
      "[after batch 250] loss = 0.001 params = {'ws': [0.487], 'bs': [-0.799]}\n",
      "[after batch 300] loss = 0.002 params = {'ws': [0.49], 'bs': [-0.816]}\n",
      "[after batch 350] loss = 0.000 params = {'ws': [0.494], 'bs': [-0.826]}\n",
      "[after batch 400] loss = 0.000 params = {'ws': [0.498], 'bs': [-0.835]}\n",
      "[after batch 450] loss = 0.001 params = {'ws': [0.503], 'bs': [-0.841]}\n",
      "[after batch 500] loss = 0.001 params = {'ws': [0.509], 'bs': [-0.845]}\n"
     ]
    }
   ],
   "source": [
    "py_params = get_params(py_dnn)\n",
    "py_loss = nn.MSELoss()\n",
    "py_opt = optim.SGD(py_dnn.parameters(), lr=lr)\n",
    "py_dnn.train()\n",
    "batch_idx = 0\n",
    "\n",
    "print('[before training]\\t{}'.format(format_params(py_params)))\n",
    "for _, (x, y) in enumerate(dataloader):\n",
    "    py_opt.zero_grad()              # in PyTorch the gradients are accumulated, so we zero them before each epoch\n",
    "    out = py_dnn(x)                 # forward pass over the network\n",
    "    loss = py_loss(out, y)          # compute the loss\n",
    "    loss.backward()                 # compute the gradients\n",
    "    py_opt.step()                   # update model parameters\n",
    "    batch_idx += 1\n",
    "    py_params = get_params(py_dnn)\n",
    "    if batch_idx % epoch_size == 0:\n",
    "        print('[after batch {}] loss = {:.3f} {}'.format(batch_idx, loss.item(), format_params(py_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train OurDNN model\n",
    "...and train the DNN model we implemented from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before training]\tparams = {'ws': [0.547], 'bs': [-0.399]}\n",
      "[after batch 50] loss = 0.021 params = {'ws': [0.513], 'bs': [-0.58]}\n",
      "[after batch 100] loss = 0.019 params = {'ws': [0.501], 'bs': [-0.677]}\n",
      "[after batch 150] loss = 0.005 params = {'ws': [0.488], 'bs': [-0.736]}\n",
      "[after batch 200] loss = 0.003 params = {'ws': [0.487], 'bs': [-0.775]}\n",
      "[after batch 250] loss = 0.001 params = {'ws': [0.487], 'bs': [-0.799]}\n",
      "[after batch 300] loss = 0.002 params = {'ws': [0.49], 'bs': [-0.816]}\n",
      "[after batch 350] loss = 0.000 params = {'ws': [0.494], 'bs': [-0.826]}\n",
      "[after batch 400] loss = 0.000 params = {'ws': [0.498], 'bs': [-0.835]}\n",
      "[after batch 450] loss = 0.001 params = {'ws': [0.503], 'bs': [-0.841]}\n",
      "[after batch 500] loss = 0.001 params = {'ws': [0.509], 'bs': [-0.845]}\n"
     ]
    }
   ],
   "source": [
    "our_params = get_params(our_dnn)\n",
    "our_loss = OurMSELoss()\n",
    "batch_idx = 0\n",
    "\n",
    "print('[before training]\\t{}'.format(format_params(our_params)))\n",
    "for _, (x, y) in enumerate(dataloader):\n",
    "    out = our_dnn.forward(x.item())         # forward pass over the network\n",
    "    loss = our_loss.forward(out, y.item())  # compute the loss\n",
    "    grad = our_loss.backward()              # compute the gradients with respect to the model output\n",
    "    our_dnn.backward(grad, lr)              # compute the gradients and update model parameters\n",
    "    batch_idx += 1\n",
    "    our_params = get_params(our_dnn)\n",
    "    if batch_idx % epoch_size == 0:\n",
    "        print('[after batch {}] loss = {:.3f} {}'.format(batch_idx, loss, format_params(our_params)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
